
\section{Unity ML-Agents}\label{appendix:mlagents}

% \subsection{Visual Encoders}\label{appendix:visual-encoders}
% todo 

\subsection{PPO Trainer: Hyperparameters}\label{appendix:ppo-trainer}

\begin{longtable}{@{} p{3.5cm} p{2.5cm} p{2.5cm} @{}} \toprule
\textbf{Hyperparemeter}       & \textbf{Default Value} & \textbf{Typical Range} \\ \midrule
visual encoder type         & simple    & - \\ 
normalize                   & false     & - \\
learning rate               & 3e-4      & 1e-5 - 1e-3 \\ 
learning rate schedule      & linear    & linear, constant \\ 
batch size                  &  1024     & 512 - 5120 \\
max steps                   &  500000   & 5e5 - 1e7 \\ 
buffer size                 & 10240     & 2048 - 409600 \\ 
time horizon                &  64       & 32 - 2048  \\
number of layers            &  2        & 1-3 \\
hidden units                &  128      & 32-512 \\
beta                        &  5.0e-3   & 1e-4 - 1e-2  \\
epsilon                     &  0.2      & 0.1 - 0.3  \\
lambda                      &  0.95     & 0.9 - 0.95  \\
number of epochs            & 3         & 3 - 10 \\ \bottomrule
% number of epochs                        & ROSTeat node is running. \\ \cmidrule{1-2} 
% Exceptions             & Image is not RGB \\ \bottomrule
\caption{Default hyperparameters in PPO trainer, taken from \cite{github-unity-mlagents-toolkit}} \label{tab:default-hyperparameters}
% \\
\end{longtable}

\newpage

\subsection{Unity Project Class Diagram}

% TODO insert C\# model diagram.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/ClassDiagram2.png}
    \caption{Class Diagram for the C\# implementation for this thesis shows the hierarchical behavior inheritance in the implementation of the agents. Generated using Visual Studio 2019.}
    \label{fig:classdiagram}
\end{figure}


\newpage
% \subsection{Example Training Configuration File}\label{appendix:ppo-trainer}
% \newpage
\input{tables/appendix_multicol_config_sample}

\newpage

\subsection{Agent Behaviors UI}  \label{appendix:agents_explanations}


% \begin{figure}[!ht]
%     \centering
%     \subfigure{\includegraphics[width=0.49\textwidth]{images/unity-environments-training-behavior-octree2.png}} 
%     \subfigure{\includegraphics[width=0.49\textwidth]{images/unity-environments-training-behavior-voxel.png}} 
%     \caption{Sample 3D assets for scenario proposals. Taken from \cite{unity-asset-store}.}
%     \label{fig:unity-my-unity-behaviors}
% \end{figure}

\begin{figure}[!ht]
    \includegraphics[width=1\textwidth]{images/unity-environments-training-behavior-octree2.png}
    \caption{Octree nodes and agent perspective in the training environment.}
\end{figure}

\begin{figure}[!ht]
    \includegraphics[width=1\textwidth]{images/unity-environments-training-behavior-voxel.png}
    \caption{Agent perspective in the training environment of the voxelized scan goal, including the nearest object compass, object detector camera, voxels-only camera and performance metrics.
    }
    \end{figure}

\newpage 
\subsection{Influence of Variables}  \label{appendix:agent_variables_influence}
    These charts also suggest that the lingering penalty has a smaller influence on the performance of agents, regardless of different strengths of the voxel reward. 

   \begin{figure}[!h]
        \centering
        \includegraphics[width=1\textwidth]{images/results_variables_obj3_detailed.png} 
        \caption{Comparison of the influence of relevant variables on the total amount of objects scanned for all object-focused runs. The variables for \textit{pigeon} and \textit{pathak} are always set to 1 (active).}
        \label{fig:results_variables_obj}
        \end{figure}
        


   \begin{figure}[!h]
        \centering
        \includegraphics[width=1\textwidth]{images/results_variables_env3.png} 
        \caption{Comparison of the influence of relevant variables on the visited volume for all environment-focused runs.}
        \label{fig:results_variables_obj}
        \end{figure}
        
        

\newpage
\subsection{In-depth Metrics}\label{appendix:indepth-metrics}
The following in-depth metrics were collected per agent

% The full list of in-depth metrics is listed below:
    \begin{multicols}{3}
\begin{itemize}
    \item name
    \item speed
    \item linger
    \item pigeon
    \item pathak
    \item constrained
    \item node\_size
    \item train\_octree\_discovery
    \item train\_voxel\_discovery
    \item voxel\_reward\_str
    \item voxel\_vision
    \item oracle
    \item train\_entropy
    \item sac
    \item n\_leafNodes
    \item n\_leafNodes\_y
    \item leafNodeVolume
    \item \%leafNodesExplored
    \item \%scanNodesExplored
    \item visual\_analysis %\_confirms\_best
    \item \%episode\_length
    \item avg\_total\_objects\_scanned
\end{itemize}
\end{multicols}


\subsection{Naming}

Following what was explained in Section \ref{chap:4:behaviors}, the run names that have a number such as \textit{run63++025} indicate that the voxel reward has a weight factor of 25\%, whereas \textit{run63++100} provides the agent a 100\% of the voxel reward. This allowed the analysis of different influences of voxel reward values and their impact on the agent's behavior. Furthermore, \textit{-constrained} indicates that the training environment manager will not reset the episode if the agent hits a boundary wall.

Similarly, following what was presented in equation \ref{eu_lingeringpenaltystrength}, agents that are labeled with the -entropy suffix, such as \textit{voxel-entropy++100}, observe the variable \textit{lingering-penalty-strength} and accordingly modify the lingering penalty at each timestep. This should enable agents to navigate longer in locations with high entropy. In contrast, agents with the suffix \textit{-noTrainEntropy} only introduce observe the strength factor but do not modify the lingering penalty. 

Given its performance, the baseline for mixed-focus agents is also known as \textit{voxel-entropy-nospeed-octree-pigeon-pathak}. 
As mentioned in \ref{appendix:agents_explanations}, the variants of this baseline are created varying the octree leaf node size, the lingering penalty and the "noTrainEntropy" suffix, which refers to agents that receive entropy observations yet do not modify the lingering penalty based on the entropy levels.

The following tables further explain the names of each agent in terms of rewards and observations.
\input{tables/appendix_agent_def_by_obs}
\input{tables/appendix_agent_def_by_rewards}

\input{tables/appendix_voxel_focused_runs}
\input{tables/appendix_octree_focused_runs}
\input{tables/appendix_mix_focused_runs}
\input{tables/appendix_small_env_runs}


\newpage


\subsection{Small Environment Results (Plots)}
\begin{figure}[!ht]
        \centering
        \includegraphics[width=1\textwidth]{images/results_baseAgent.png}
        \caption{Small environment (32 $m^2$) results.) .
        }
        \label{fig:results-small-env-performances}
\end{figure}

