\section{Related Work}\label{chap2:related-work}
We study the problem of how to maximize simulated robotic exploration to eventually find objects and scan them thoroughly. This refers to techniques on exploration and coverage in embodied contexts. Embodied contexts are those that take into account an agent’s physical and cognitive abilities. This is slightly related to our previous work on milking robots, where their limitations serve as out initial motivation. Accordingly, it is tightly related to 3D vision (what is currently possible to understand from images), active SLAM in robotics (what are navigation policies and perception methods) and intrinsic motivation (picking what sections of an environment to explore). Related efforts are surveyed below.

\textbf{Milking Robots.}
Given that current laser technology used in cow milking robots limits the performance of the systems, a considerable technological growth is possible by leveraging the latest technological advancements. As stated by \textcite{pal2017algorithm}, laser technology is not capable of differentiating between a cow's teat and a leg, therefore manipulating the suction cups in the wrong direction. Along these lines, \citeauthor{pal2017algorithm} contribute by proposing a fast and reliable solution to the problem of 3D pose recognition of cow teats using TOF \cite{terabee2021tofprinciple}, RGB-D \cite{cruz2012kinect} and Thermal Imaging \cite{aaa2021thermalimaging}. However, even though their pipeline provides a new accurate way for estimating the teats' poses, it lacks any intelligence with respect to knowing what is a cow teat (semantics).
\textcite{rastogi2019teat} take a similar stand against the limitations of laser assisted edge detection technologies, where current solutions cannot differentiate between a healthy and a diseased teat. To improve on this, they propose two functional yet limited alternatives to the task: a Haar-cascade classifier and a YOLO classifier for cow teats, approaches which work on real time but lack reliable accuracy. The Haar cascade classifier fails to detect any teats in the occluded tests, and YOLO fails to detect a fourth teat, even with a prediction threshold of 0.5.
% Even though YOLO performs splendidly and even in real-time, YOLO provides only a bounding box for the object found, whereas a segmentation network benefits by separating the pixels of salient object from the background.
% left out
% Additionally, \textcite{dorokhov2019recognition} analyzed the study from \textcite{akhloufi20143d} for vision systems for livestock and gathered that RGB-D technologies are preferable to ToF. Regardless, they propose a k-nearest neighbors approach from the point cloud captured by a ToF camera. They also state that the vacuum action in the teat attachment step allows for the teat pose estimation error to be at most 1 cm. They fail, however, to provide any benchmarks about the performance of their algorithm.
In more general terms, \textcite{o20193d} review 3D computer vision systems and techniques for precision dairy farming. More specifically, by looking at Time of Flight and streoscopic vision systems and conclude that robust systems which adapt to weather conditions, herd characteristics, farmyard layout, etc., (generally unknown scenarios) are required. Hence they foresee that the future state of the art technologies use Geometric Deep Learning for processing data in non-Eucledian domain, such as graphs and manifolds. On this note, \textcite{cao2020comprehensive} provide a comprehensive review of deep learning methods in the graph and manifold domain, including history, background, applications and benchmark datasets as reference for research in geometric deep learning. First, this thesis' work aims to provide a solution to the milking robot uninformativeness problem, by allowing exploration of the environment around the cow's udder, so that hidden teats are not ignored. Second, it aims to also contribute to other fields such as surveillance and discovery, such as car accidents in police investigations, inspection drones from fire departments, etc.  

% In terms of robotic grasping, \textcite{ciocarlie2014towards} propose a complete software achitecture for reliable grasping of household objects. Their work combines scene interpretation from 3D data, grasp planning, motion planning and failure identification and recovery modules. Related to this work, their semantic perception approach uses Eucledian clustering on pre-segmented objects from a scene from a 3D point cloud. They then use a similar technique to the ICP algorithm to match the object clusters to a database of pre-defined 3D models. One of their main concerns for future improvements is that robots should be able to grasp objects in unknown scenarios.
% Moreover, \textcite{manuelli2019kpam} state that manipulation policies should generalize to potentially unknown instances. Therefore, they introduce a novel category-level manipulation pipeline that uses semantic 3D keypoints for object representation and enables the specification for robot action planning and grasping with centimeter level precision. 
% Finally, \textcite{arad2020development} propose an extensively tested and integrated system design for SWEEPER, a robot for harvesting sweet peper fruit in greenhouses. SWEEPER uses time-of-flight technologies and a single sensor for RGB and depth analysis, which combined with a shape and color-based detection algorithm allows high frame-rate operation. To summarize, they use they use traditional-computer-vision-enabled pipeline to analyze the morphology of the shapes recognized, and then compare its relative size to an average pepper size, which allows fast calculations. 
% % , which is capable of autonomously driving on irregular floors. 
\textbf{3D Vision.}
% 3D vision
Given the technological lacklustre in current solutions implemented in milking robots, it is imperative to look into current state of the art of 3D vision, in order to bridge the gap between research and applications. 
%
Several works have been grabbing inspiration from biological vision and the brain's physiology by separating the "what" from the "where" \cite{medathati2016bio, ebrahimpour2019ventral}. Others include the concept of information over time to their proposals to not only study the behavior of familiarity given seen objects in the brain, but also how the temporal dimension can improve action recognition and allow accurate object classification \cite{wagatsuma2018locus, simonyan2014two, diba2017temporal, hou2019efficient}. Accordingly, the interpretation of the information present in our environments, whether through RGBD data, sparse point clouds, etc., specially in the presence of heavy occlusion, and the generation of semantic features from it is deeply studied in many works, including for practical setups such as industrial inventories \cite{qi2018frustum, qi2017pointnet, qi2017pointnet++, nivaggioli2019using}.
% 

In the field of robotics, 
\textcite{lin2020using} provide a segmentation-based architecture that leverages the concept of primitive shapes to reduce object shapes to known grasps based on these simpler shapes. 
Inspired by two-stream hypothesis of visual reasoning, \textcite{jang2017end} present a semantic grasping framework that learns object detection, classification and grasp-planning in an end to end fashion. Their ventral stream recognizes the objects class and the dorsal stream interprets the geometric relationships necessary to execute successful grasps.
% \cite{cheng2018reinforcement}
Similarly, \textcite{cheng2018reinforcement} propose an architecture using active vision for manipulating objects under occlusions. More specifically, they use artificial agents to learn gripper and camera control policies using reinforcement learning in the presence of occlusions. They propose hand-eye controllers that learn how to move the camera to keep the object within the field of view. In lines with curriculum learning research, they show evidence that agents for both policies and object detection provide better performance when initially trained in an environment without occlusions.
Another work worth mentioning is the approach proposed by \textcite{levine2018learning}, which learns successful eye-hand coordination for robotic grasping on novel objects using CNNs and doesn't require camera calibration. They use a continous feedback loop to correct mistakes and suggest to use reinforcement learning as future work to learn a wider variety of grasp strategies. 
% \textcite{james2020rlbench} contribute to the robotics research in reinforcement learning by presenting RLBench, a unified benchmark aimed to aid robotic researchers in a variety of tasks such as manipulation, reinforcement learning, imitation learning, geometric computer vision, multi-task learning and few-show learning.

These works provide useful insight into camera intrinsics and extrinsics, robotic movement mechanics and limitations of each approach. However, pre-processing and manipulation of high-level RGBD data continue to be memory expensive and computationally complex. This thesis' work proposes an efficient voxel-based approach that reduces the complexity in the environments and allows exploration and understanding of 3D environments, which to the best of our knowledge hasn't been exploited before for tasks aimed at exploration of environments and objects.

\textbf{Navigation in Classical Robots.} % (learning policies, chen, 2019)
As described extensively by \textcite{chen2019learning}, classical robotic tasks attempt to build a map of their environments and then apply a path planning algorithm to reach some destination. Work in this direction has been extensively studied \cite{zisserman2004multiple, thrun2002probabilistic, lavalle2006planning}. The research mentioned, however, follows mostly a passive SLAM approach, whereas an active SLAM approach to discover an environment is less studied. Chen summarizes key efforts in active SLAM, contributing to Cadena's \cite{cadena2016past} extensive literature: active SLAM has been formulated as a partially observable Markov decision process (POMDP) \cite{martinez2009bayesian} or as choosing actions that reduce uncertainty in the environment's mapping \cite{carrillo2012comparison}. These methods rely on sensors for their measurements and are highly affected by noise and view the exploration problem as a geometry problem, ignoring the semantics the environment could provide (e.g. doors). \textcite{chen2019learning} contribute to this research by proposing a learning-based approach, investigating different policy architectures, reward functions and training paradigms, outperforming classical geometry-based approaches and generic learning-based exploration techniques.

\textbf{Exploration for Navigation.} % (learning policies, chen, 2019) THIS IS USING RL for exploration
Orthogonal works \cite{gandhilearning, sadeghi2rl} to \textcite{chen2019learning}'s learn policies on how to avoid collision and prefer open space than maximizing environment coverage without leveraging from their learned maps, even mimicking SLAM techniques \cite{zhang2017neural}. Similarly, many works study reinforcement-learning-based exploration \cite{schmidhuber1991possibility, stadie2015incentivizing, pathak2017curiosity, fu2017ex2, lopes2012exploration, singh2005intrinsically}, proposing intrinsic reward functions that prefer novel states. 
% BIG SENTENCE
% The difference in this master's thesis work lies in the added 3D dimension and the realism of the environments. Furthermore 
This work proposes a variety of exploration agent variants using extrinsic rewards and analyzes the influence of intrinsic curiosity \cite{pathak2017curiosity}. Similar to Chen's work, this thesis studies how learning-based techniques can be exploited in real world deployments. 

Other related efforts use 
\cite{jayaraman2018learning} reward function based on pixel reconstruction to learn how to explore and how to solve tasks. 
% However, they do it in context of 360 images, and their precise reward can’t be estimated intrinsically.
% \textcite{xu2017autonomous} generate smooth movement path for high-quality camera scan by using time-varying tensor ﬁelds.
\textcite{bai2016information} study Gaussian Process regression in an information-theoretic exploration method tested on simplistic environments.
\textcite{kollar2008trajectory} learn a trajectory that maximizes the accuracy of the SLAM-derived map when compared to the assumed ground-truth map.
% BIG SENTENCE
In contrast, this thesis' learning policy mandates in real-time the action the agent should take when balancing two interests: using a 3D map to prefer open undiscovered spaces and exploring voxels in salient objects. 

% \textbf{Learning for Navigation.} % (learning policies, chen, 2019) THIS IS USING SEMANTIC CUES FOR NAVIGATION

\textbf{Active Vision.} % (semantic curiosity, chaplot 2020)
The topic of active vision has been actively studied in the fields of services medicine industry and agriculture, and it involves a robot which analyzes its environment and its own state to collect multiple views to acquire more information about the environment itself \cite{zeng2020view}. An alternative term used by Chaplot is \textit{active perception} \cite{bajcsy1988active}, which refers to "the problem of actively moving the sensors around at test time to improve performance on the task by gaining more information about the environment", which has been applied in a variety of applications such as object detection \cite{ammirato2017dataset}, amodal object detection \cite{yang2019embodied}, scene completion
\cite{jayaraman2018learning}, and localization \cite{chaplot2018active, fox1998active}. 

In other words, visual tasks like object recognition and reconstruction, scene exploration, target tracking, etc., can benefit substantially from the proposal of active vision approaches for "more information given more views". 
In theory, the promise of more information to reduce the uncertainty about our environment and the characteristics of the objects in it could provide great value, but in practice this is not so easily implemented \cite{zeng2020view}.

The most important factors that discourage these model-based approaches are the complexity of real-world models (which are biased in some way) \cite{wang2018look}, imprecision of motion mechanics, unreliability of visual perception, general uncertainty about the environment and the changing task requirements \cite{vasquez2017view, palomeras2019autonomous}. For more information on active vision, please refer to \textcite{zeng2020view}'s survey.

In this work, we focus on the following: general uncertainty about the environment, unreliability of visual perception, a biased model of reality that limits the dynamics present in such environment and the changing task requirements. Firstly, we tackle the problem of general uncertainty in an environment through octree-based exploration. Secondly, we avoid unreliable visual detectors by reducing the uncertainty about objects in an environment through perception voxelized structures. Third, motivated by Chaplot's extensive work \cite{chaplot2017arnold, chaplot2018gated, chaplot2020learning, chaplot2020neural, chaplot2020semantic}, we achieve these agent behaviors through a model-free approach using a self-supervised policy and does not rely on end-task reward \cite{schulman2017proximal, sung2018exploring}, in contrast to other approaches \cite{chaplot2018active, jayaraman2018learning, yang2019embodied}. Fourth, our baseline behavior policy for the exploration of environments and objects is applicable to a multitude of tasks and further scenarios, from rescue missions or danger assessment to product quality assurance. Finally, we attack the hidden disease of reproducible research by exploiting Unity 3D simulated environments for modeling, training, testing, benchmarking and further use case evaluations.



% We consider the problem in a different setting and study how to efficiently move around to best learn a model. 

\textbf{Intrinsic Rewards.} % (semantic curiosity, chaplot 2020)
This thesis' work is tightly related to exploration using reinforcement learning \cite{auer2002using, jaksch2010near, schmidhuber1991possibility, sutton2018reinforcement}. 
The mentioned works set up the problem as a Markov Decision Process which attempts to learn high reward paths, by motivating intrinsic rewards which direct the agents toward previously unseen \cite{eysenbach2018diversity} or unknown spaces \cite{pathak2017curiosity} in the environment. 
%
This thesis' work not only motivates the agent to visit unknown spaces but also rewards it for scanning voxels in the environment. This takes a different direction from the related work since it does not utilise semantics to define objects of interest but voxels as a less complex form of the inherent structure of objects. Moreover, in contrast to the work by \cite{chaplot2020semantic}, the focus lies in exhaustive exploration as a baseline trajectory discovery, where the improvement of semantic models has a secondary, tangential purpose. 

% The goal of these works is to effectively explore a
% Markov Decision Process to find high reward paths. 
% A number of works formulate
% this as a problem of maximizing an intrinsic reward function which is designed
% to incentivize the agent to seek previously unseen [19] or poorly understood [37]
% parts of the environment. This is similar to our work, as we also seek poorly
% understood parts of the environment. However, we measure this understanding
% via multi-view consistency in semantics. This is in a departure from existing
% works that measure it in 2D image space [37], or consistency among multiple
% models [38]. Furthermore, our focus is not effective exhaustive exploration, but
% exploration for the purpose of improving semantic models.

\textbf{Visual Navigation and Exploration.} % (semantic curiosity, chaplot 2020)
% to obtain a point cloud of the cow teat  attempt to tackle the problem processing the point cloud 
According to \textcite{chaplot2020semantic}, visual navigation works can be separated into two categories depending on whether the location of the goal is known or not. Navigation problems where the destination point is given, include the point-goal task \cite{gupta2017cognitive, savva2017minos}, or the vision-language \cite{anderson2018vision} task where the destination is given in natural language. These tasks do not require extensive exploration since the destination is given a priori as coordinates (explicit destination) or as a path (implicit destination). 

In contrast, navigation problems where the location of the goal is not given, include a variety of methods \cite{chaplot2020semantic}. They include: navigating to a
fixed set of objects \cite{chaplot2017arnold, dosovitskiy2016learning, gupta2017cognitive, lample2017playing, mirowski2016learning, wu2016training}, navigating to an object specified by language \cite{chaplot2018gated, hermann2017grounded} or by 
an image \cite{chaplot2020neural, zhu2017target}, and navigating to a set of objects in order to answer a question 
\cite{das2018embodied, gordon2018iqa}.
These methods must be able to explore the environment extensively to find the goals. However, some methods do not really solve the exploration problem if the agent is spawned close to the destination goal. 
They instead focus instead on other challenges; for example, models for First Person Shooter (FPS) video games  \cite{chaplot2017arnold, dosovitskiy2016learning, lample2017playing,  wu2016training} are able to train reactive agents to avoid obstacles and enemies, collective sporadic rewards scattered across the environment. Accordingly, other models focus on recognizing the goal (visual perception) \cite{chaplot2020neural, zhu2017target}, understanding the goal through language \cite{chaplot2018gated, hermann2017grounded} or understanding the visual properties which make up the goal \cite{das2018embodied, gordon2018iqa}. Reinforcement-learning is successfully applied in these challenges but exhaustive exploration, especially in large environments, remains to be studied with more depth. 

%
One of the most relevant related the efforts for this thesis is the work done by \textcite{parrot2022anafiai} on their newest flying drone \textit{ANAFI Ai} in 2021. The latest version of the ANAFI Ai is capable of autonomous flight and obstacle avoidance, with an improved dual vision system for added stabilization and increasedaccuracy of the perceived depth maps. They achieve navigation and obstacle avoidance by mapping depth maps into a occupancy grid, therefore simplifying environment around the drone to navigate. Parrot also enables the possibility for developers to write custom flight missions.
% The main difference to this work is that we use model-free policies not only to explore environments but also to reduce the uncertainty about possible objects in the environment given voxel structures of such objects.
This thesis' work goes in a similar direction, where we voxelize the agent's perceived environment. However, our method aims to maximize the explored area \cite{chaplot2020learning, chen2019learning, fang2019scene} using the octree nodes as a part of the reward signal and further exploits the benefit of voxels and semantic entropy to model a visual agnostic behavior to reduce the uncertainty about objects. Furthermore, our method proves its applicability to a multitude of real world scenarios, using realistic 3D scenarios in Unity as proof of concept.

% and that the agent's reward function novelty is two-fold: it leverages a 3D map to reward the agent, based on novel spaces, and it exploits the voxels found in the environment to scan goals in more depth.



% \section{Vision Systems in Automated Milking Robots}\label{chap:2:melkroboter}
% half a page

% Vision Systems in Automated Milking Robots

% The systems available on the market all consist of a manipulator and the associated basic equipment. Therefore, when using several manipulators at the same time, the basic equipment must also be present several times. The possibility of operating several manipulators on one base unit (the base unit contains the pumping and cooling units as well as the tanks for the milk) is not in itself a technical-scientific innovation. However, when several manipulators are used, the cost of the individual manipulator plays a significant role in the cost-effectiveness of the overall system. Therefore, the aim is to develop a manipulator that is cost-effective in terms of production, maintenance and operation, using state-of-the-art components. The content of this research project is limited only to the development of a new manipulator. The development of the complete system will be done by SLG outside this research project - the corresponding know-how is available due to years of experience in the development, production and maintenance of own milking systems. The in-house development of the overall system also allows SLG to determine all parameters of the milking process itself and thus adapt them to the specific conditions in Switzerland. The latter is not possible with the systems currently available.

% The robotic milking systems available on the market apply different strategies to detect the cows' teats. The goal is, after the cow has entered the milking parlor and the teats have been hygienically cleaned and stimulated, to apply the teat cups to the teats as quickly as possible. This involves moving the teat cup in front of the teat tip and then moving it in the direction of the teat towards the udder. Due to a vacuum, the teat cup then remains attached to the teat by itself until the vacuum is switched off after the milking process has been completed. Depending on the system, the cup is then removed by pulling back the hose hanging from the cup or by pulling on a separately attached rope.

% The attachment of the four buckets is done in the case of the Lely machines by moving a platform where all four buckets are moved simultaneously until they are attached. This design has the advantage that the
% This design has the advantage that the robot arm only has to travel the long way under the cow once per milking operation, and only short movements are then required for the attachment of the teat cups. In addition, if the cups do not stick to the teat, they fall back onto the platform and not onto the floor, where they become dirty and a time-consuming cleaning process becomes necessary. Thus, the docking time for these systems is typically about one minute for all four cups. However, this design also has drawbacks: The somewhat bulky design means that more volume must be moved between the cows' legs. This is uncomfortable for the cow and also means an increased risk of the cow stepping on or into the robot arm. In addition, the large mass of the platform also means that it is difficult to constantly follow the teat cups to the teat during the docking maneuver. Other products, such as DeLaval's VMS, use the robot to move each cup individually from a magazine next to the cow box to the teat. These manipulators can be built slimmer and lighter, allowing them to move more agilely. The disadvantage is that the docking process for four cups takes longer (about two minutes) and also that if the cup does not adhere properly to the teat, it will fall to the floor under the cow, requiring an additional cleaning procedure.

% The existing milking robots measure the position of the teats once before starting the docking procedure and then approach the teats in a purely position-controlled manner. In contrast, the new manipulator will be designed in such a way that it is able to detect the teats during the entire docking process in order to always be able to move the teat cup in the direction of the teat. This guarantees that even if the cow moves during the docking process, the teat is reached as quickly as possible and with a high degree of safety. In this way, the robustness and also the time of the docking process of the teat cups can be significantly improved. For this, in addition to the appropriate 3D sensor technology, a corresponding real-time evaluation algorithm is required. Furthermore, the manipulator itself must be designed in such a way that it can dynamically move a platform with the four teat cups accordingly. Last but not least, the control system must also be capable of executing sensor-guided movements in real time.

% Even if some competitors have equipped their latest milking robot models with more modern 3D sensors in the meantime, the sensor-guided dynamic target movement described here has not been implemented, among other things because the necessary manipulator design is not available.

